# model
pre_train_model_path: "../../pre_trained_model/NLP/deberta-v3-large"
out_size: 6
dropout_rate: [0.1, 0.2, 0.3]

# data
batch_size: 2
seq_len: 640
num_workers: 0
train_data_path: "./data/ori_train.csv"
test_data_path: "./data/test.csv"
target_cols: ["cohesion", "syntax", "vocabulary", "phraseology", "grammar", "conventions"]

# training
weight_decay: 1.0e-6
lr: 2.0e-5
min_lr: 1.0e-7
epochs: 20
metric_key: "MCRMSE"
patience: 5
max_norm: 10
enabel_awp: false
adv_lr: 1.0e-3    # 对抗训练lr
adv_eps: 1.0e-3   # 对抗训练eps
gradient_accumulation_steps: 1
n_fold: 1
continue_training: true

# save
model_save_path: "./saved_model/deberta-v3-large_0919/"
